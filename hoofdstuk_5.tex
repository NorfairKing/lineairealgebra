\documentclass[lineaire_algebra_oplossingen.tex]{subfiles}
\begin{document}

\chapter{Hoofdstuk 5}
\section{Bewijzen uit het boek}

\subsection{Stelling 5.2 p 177}
Zij $A\in \mathbb{R}^{n\times n}$ een vierkante matrix.
\subsubsection*{Te Bewijzen}
\begin{center}
Een getal $\lambda\in\mathbb{R}$ is een eigenwaarde van $A$.
\end{center}
\[\Leftrightarrow\]
\begin{center}
$\lambda$ is een nulpunt van de karakteristieke veelterm $det(X\mathbb{I}_n - A)$ van $A$
\end{center}
\subsubsection*{Bewijs}
\begin{proof}
Bewijs van een equivalentie.
\begin{itemize}
\item $\Rightarrow$\\
Omdat $\lambda$ een eigenwaarde is van $A$ bestaat er een (eigen)vector $v$ zodat de volgende bewering geldt\footnote{Zie Definitie 5.1 p 177}.
\[
A\cdot v = \lambda v
\]
We weten dat $\lambda v =  \lambda \mathbb{I}_n \cdot v$ en dat de matrixvermenigvuldiging distributief is als ze bepaald is\footnote{Zie Eigenschappen 1.22 b}.
\[
A\cdot v - \lambda \mathbb{I}_n \cdot v = \vec{0} = (A-\lambda\mathbb{I}_n)\cdot v = \vec{0}
\]
Omdat $v$ per definitie geen nulvector is moet de determinant van $(A-\lambda\mathbb{I}_n)$ nul zijn opdat opdat $(A-\lambda\mathbb{I}_n)\cdot v = \vec{0}$ geldt.

\item $\Leftarrow$\\
Als $det(A-\lambda\mathbb{I}_n) = 0$ geldt voor $\lambda$ met $v$ als eigenvector, dan geldt zeker het volgende.
\[
(A-\lambda\mathbb{I}_n)\cdot v = \vec{0}
\]
\end{itemize}
\end{proof}

\subsection{Voorbeeld 5.4 p 178}
\subsubsection*{1)}
We zoeken nog een oplossing van de volgende vergelijking.
\[
\begin{pmatrix}
-1 & -1\\
-1 & -1\\
\end{pmatrix}
\cdot
\begin{pmatrix}
x\\y
\end{pmatrix}
=
\vec{0}
\]
De oplossingsverzameling hiervan is de volgende.
\[
V = \{-\lambda,\lambda|\lambda\in\mathbb{R}\}
\]
Als we nu $\lambda = 1$ kiezen krijgen we als eigenvector bij voorbeeld $(-1,1)$.

\subsubsection*{5)}
Voor elke eigenvector $v$ van $D$ geldt dat de afgeleide van $v$ een veelvoud is van $v$. Elke constante functie is een eigenvector en $0$ is de eigenwaarde voor die functies.

\subsection{Over Definitie 5.6 p 181}
Dit houdt in dat er voor $A$ een inverteerbare matrix $P$ bestaat zodat. $A = P^{-1}\cdot B\cdot P$ waarbij $B$ een diagonaalmatrix is.

\subsection{Stelling 5.7 p 181}
Zij $A \in \mathbb{R}^{n\times n}$ een vierkante matrix.
\subsubsection*{Te Bewijzen}
\begin{center}
$A$ is diagonaliseerbaar.
\end{center}
\[\Leftrightarrow\]
\begin{center}
$A$ heeft een basis die volledig bestaat uit eigenvectoren van $A$.
\end{center}
\subsubsection*{Bewijs}
\begin{proof}
Bewijs van een equivalentie.
\begin{itemize}
\item $\Rightarrow$
Omdat $A$ diagonaliseerbaar is bestaat er een inverteerbare matrix $P$ zodat $A = P \cdot \Lambda\cdot P^{-1}$ geldt met $B$ een diagonaalmatrix. $\Lambda$ ziet er dus als volgt uit.
\[
\Lambda =
\begin{pmatrix}
\lambda_1 & 0 & \cdots & 0\\
0 & \lambda_2 & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots\\
0 & 0 & \cdots & \lambda_n
\end{pmatrix}
\]
Nu geldt $A \cdot P = P \cdot \Lambda$ omwille van de definitie van inverteerbaarheid\footnote{Zie Definitie 1.27 p 34}.
We beschouwen $P$ nu als een rij van kolommen $P=(p_1 p_2 \cdots p_n)$
$A\cdot P$ ziet er dus als volgt uit.
\[
A\cdot P = A(p_1 p_2 \cdots p_n) = (Ap_1 Ap_2 \cdots Ap_n)
\]
Bovendien geldt de volgende bewering omdat $A \cdot P = P \cdot \Lambda$ geldt.
\[
A(p_1 p_2 \cdots p_n) = (Ap_1 Ap_2 \cdots Ap_n) = (p_1 p_2 \cdots p_n)\Lambda = (\lambda_1p_1 \lambda_2p_2 \cdots \lambda_np_n)
\]
Elk van de kolommen in $(\lambda_1p_1 \lambda_2p_2 \cdots \lambda_np_n)$ voldoet dus aan de volgende vergelijking omdat $\Lambda$ een diagonaalmatrix is.
\[
Ap_i = \lambda_ip_i
\]
Elke kolom van $P$ is bijgevolg een eigenvector van $A$ de eigenwaarden die horen bij deze eigenvectoren staan dan in de overeenkomstige kolommen op de diagonaal van $\Lambda$.
Nu moeten we nog bewijzen dat de kolommen van $P$ een basis vormen voor $\mathbb{R}^n$
Omdat $P$ precies $n$ kolommen bevat moeten we enkel bewijzen dat de kolommen van $P$ lineair onafhankelijk of dat ze voortbrengend zijn\footnote{Zie Stelling 3.41 p 109}. Omdat $P$ inverteerbaar is geldt dat de determinant van $P$ niet nul is\footnote{Zie Stelling 2.4 p 59}. Bijgevolg zijn de kolommen van $P$ lineair onafhankelijk\footnote{Stelling 2.2 p 57 en Zie Stelling 2.3 p 58} omdat $P$ rijreduceerbaar is tot een matrix zonder nulrijen.

\item $\Leftarrow$
$\mathbb{R}^n$ heeft een basis volledig bestaand uit eigenvectoren van $A$. Noem deze basis $\beta = \{\beta_1,\beta_2,...,\beta_n\}$. De vectoren uit deze basis voldoen dus aan de volgende bewering.
\[
A\cdot \beta_i = \lambda_i\beta_i
\]
$A$ beschouwen we als de matrix van een lineaire afbeelding ten opzichte van de standaard basis.
De matrix van basisverandering van de basis van de eigenvectoren naar de standaardbasis noemen we $P$. Dit is precies de matrix waarin de eigenvectoren van $A$ in kolommen staan.
$P^{-1}$ is nu de matrix van basisverandering van de standaardmatrix naar de basis van eigenvectoren. $P$ is zeker inverteerbaar omdat de kolommen lineair onafhankelijk zijn. We weten nu dat $A = PBP^{-1}$ geldt\footnote{Zie pagina 150 voor meer uitleg.}. Vermenigvuldig nu $P$ rechts aan beide kanten.
\[
AP = PB
\]
Beschouw $P = (\beta_1 \beta_2 \cdots \beta _n)$ als een rij van kolommen $\beta_i$. Beschouw $B = (b_1 b_2 \cdots b_n)$ als een kolom van rijen $b_i$.
\[
AP = (A\beta_1 A\beta_2 \cdots A\beta_n) = (\lambda_1\beta_1 \lambda_2\beta_2 \cdots \lambda_n\beta_n) = PB
\]
De tweede gelijkheid geldt omdat $\beta_i$ eigenvectoren zijn.
De derde gelijkheid kan enkel gelden als $B$ een diagonaalmatrix is. Sterker nog, $\lambda_i$, de eigenwaarden van $A$ staan precies op de diagonaal van $P$.

\end{itemize}
\end{proof}

\subsection{Stelling 5.8 p 182}
Zij $A,B \in \mathbb{R}^{n\times n}$ vierkante matrices met $B = P^{-1}AP$ zodat $A$ en $B$ gelijkvormig zijn.
\subsubsection*{Te Bewijzen}
\begin{enumerate}
\item $A$ en $B$ hebben dezelfde karakteristieke veelterm.
\item $A$ en $B$ hebben dezelfde determinant.
\item $A$ en $B$ hebben hetzelfde spoor.
\end{enumerate}
\subsubsection*{Bewijs}
\begin{proof}
Direct bewijs.
\begin{enumerate}
\item 
\[
det(X\mathbb{I}_n - B) = det(X\mathbb{I}_n - P^{-1}AP) = det(XP^{-1}P - P^{-1}AP)
\]
Merk op dat $X$ een scalar is, geen vector.
We weten dat $XP^{-1}P = XP^{-1}IP P^{-1}XIP$\footnote{Zie Eigenschap 1.22 p 32 d)}. We kunnen dus verder gaan via gelijkheden.
\[
= det(P^{-1} XI P - P^{-1}AP) = det(P^{-1}\cdot ( XI P - AP)) = det(P^{-1}\cdot ( XI- A) \cdot P))
\]
De bovenstaande twee vergelijkingen gelden omwille van de distributiviteit van de matrixvermenigvuldiging ten opzichte van de matrix optelling\footnote{Zie Eigenschap 1.22 p 32 a) en b)}. De volgende gelijkheden gelden omwille van een eigenschap dan de determinantafbeelding\footnote{Zie Stelling 2.4 p 59 3} en de commutativiteit van de vermenigvuldiging in $\mathbb{R}$.
\[
=det(P^{-1})\cdot det( XI- A)\cdot det(P) =det( XI- A)\cdot det(P^{-1}) \cdot det(P)
\]
We weten dat $det( P^{-1}) = \frac{1}{det(P)}$\footnote{Zie Gevolg 2.5 p 60} dus de volgende gelijkheid geldt ook.
\[
=det( XI- A)\cdot \frac{1}{det(P)}\cdot det(P) = det( XI- A)
\]
Dat laatste rechterlid is precies de karakteristieke vergelijking van $A$.

\item
\[
B = P^{-1}AP
\]
\[
det(B) = det(P^{-1}AP) = det(P^{-1})\cdot det(A)\cdot det(P)
\]
\[
= det(A)\cdot det(P^{-1})\cdot det(P) = det(A)\cdot \frac{1}{det(P)}\cdot det(P) = det(A) \cdot 1 = det(A)
\]

\item
\[
B = P^{-1}AP 
\]
\[
Tr(B) = Tr(P^{-1}A P) = Tr(P^{-1}P A) = Tr(IA) = Tr(A)
\]
Bovenstaande gelijkheden gelden omwille van een eigenschap van het spoor van een product. Namelijk dat $Tr(AB) = Tr(BA)$ geldt\footnote{Zie opdracht 1.25 p 33}.
\end{enumerate}
\end{proof}

\subsection{Stelling 5.16 p 189}
Zij $L:V\rightarrow V$ een lineaire transformatie van de eindig dimensionale vectorruimte $(\mathbb{R},V,+)$ met spectrum $Spec(L) = \{\lambda_1,\lambda_2,...,\lambda_n\}$.
\subsubsection*{Te Bewijzen}
\begin{enumerate}
\item $m(\lambda_i) \ge 1$\\
\item $d(\lambda_i) \ge 1$\\
\item $d(\lambda_i) \le m(\lambda_i)$\\w
\end{enumerate}
\subsubsection*{Bewijs}
\begin{proof}
Bewijs door gefoefel.
Voor elke eigenwaarde $\lambda_i$ geldt het volgende.
\begin{enumerate}
\item Elke eigenwaarde heeft minstens een algebra\"ische multipliciteit van $1$. Anders zou het geen eigenwaarde zijn.

\item De eigenruimte van $\lambda_i$ is een vectorruimte van minstens dimensie $1$ omdat er voor elke eigenwaarde minstens $1$ eigenvector is, en dit niet de nulvector mag zijn.

\item
We weten dat de eigenruimte $E_{\lambda_i}$ van $\lambda_i$ dimensie $d(\lambda) = d$ heeft. Er bestaat dus een basis $\beta = v_1,v_2,...,v_d$ voor $E_\lambda$ met $d$ elementen. Omdat $\beta$ een vrij deel is van $E_\lambda$ en $V$ en omdat $E_\lambda$ een deelruimte is van V\footnote{Zie Definitie 5.14 p 188} geldt dat $\beta$ uitgebreid kan worden tot een basis van $V$\footnote{Zie Stelling 3.37 p 107}. Ten opzichte die uitgebreide basis $\beta'$ van $V$ ziet de matrix van $L$ er als volgt uit\footnote{Zie Stelling 5.7 p 181}. (Dit is het punt van eigenvectoren). ($L_{\beta'}^{\beta'}\cdot v_i$ moet $\lambda v_i$ zijn.)
\[
L_{\beta'}^{\beta'} = 
\begin{pmatrix}
\lambda & 0 & \cdots & 0 & \bullet & \cdots & \bullet\\
0 & \lambda & \cdots & 0 & \bullet & \cdots & \bullet\\
\vdots & \vdots & \ddots & \vdots & \vdots & &  \vdots\\
0 & 0 & \cdots & \lambda & \bullet & \cdots & \bullet\\
\vdots & \vdots & & \vdots & \vdots & \ddots & \vdots\\
0 & 0 &\cdots & 0 & \bullet & \cdots & \bullet\\
\end{pmatrix}
\]
We weten dat de karakteristieke veelterm onafhankelijk is van de gekozen basis\footnote{Zie Gevolg 5.9 p 182}. We beschouwen nu de karakteristieke veelterm van bovenstaande matrix.
De karakteristieke veelterm van deze matrix is dan van de volgende vorm. 
\[
\phi_L(X) = (X-\lambda)^dp(X)
\]
$m(\lambda)$ is minstens $d$ want $(X-\lambda)^d$ zorgt al voor multipliciteit $d$, maar in $p(X)$ kan $\lambda$ ook nog voorkomen. In symbolen:
\[
m(\lambda) \ge d = d(\lambda)
\]
\end{enumerate}
\end{proof}

\subsection{Stelling 5.18 p 190}
Zij $L:V\rightarrow V$ een lineaire transformatie van een eindigdimensionale vectorruimte $(\mathbb{R},V,+)$ en zij $\lambda_1,\lambda_2,...,\lambda_n$ verschillende eigenwaarden van $L$ met bijhorende eigenvectoren $v_1,v_2,...,v_n$.
\subsubsection*{Te Bewijzen}
$v_1,v_2,...,v_n$ zijn lineair onafhankelijke vectoren.
\subsubsection*{Bewijs}
\begin{proof}
Bewijs door inductie op $n$.\\\\
\emph{Stap 1: (basis stap)}\\
De bewering geldt voor $n=1$ want $\{v_i\}$ is lineair onafhankelijk. Dit is waar omdat $v_i$ geen nulvector is\footnote{Zie Definitie 5.3 p 178}.\\\\
\emph{Stap 2: (inductie stap)}\\
Stel dat de bewering geldt voor een bepaalde $n=k$ (inductiehypothese). We bewijzen nu dat daaruit volgt dat de bewering geldt voor $n=k+1$.
We beschouwen $k+1$ eigenwaarden met bijhorende eigenvectoren. Uit de inductiehypothese volgt dat elke deelverzameling van $k$ elementen van die eigenvectoren vrij is.
We bewijzen uit het ongerijmde dat wanneer we de $k+1$-ste eigenvector toevoegen aan die deelverzameling, de verzameling nog steeds vrij is.
We nemen dus aan dat $v_{k+1}$ lineaire afhankelijk is van $v_1,v_2,...,v_k$ en proberen tot een contradictie te komen.
\[
\exists \mu_i\in\mathbb{R} : v_{k+1} = \sum_{i=1}^k\mu_iv_i
\]
Nemen we nu van beide kanten de afbeelding $L$ dan bekomen we de volgende vergelijking.
\[
L( v_{k+1}) = L\left(\sum_{i=1}^k\mu_iv_i\right)
\]
L is lineair\footnote{Zie Lemma 4.2 p 130} en $v_i$ zijn eigenvectoren met eigenwaarden $\lambda_i$ (gegeven).
\[
L(v_{k+1}) = \sum_{i=1}^k\mu_iL(v_i) = \sum_{i=1}^k\mu_i\lambda_iv_i
\]
Bovendien is $v_{k+1}$ ook een eigenvector van $L$.
\[
L(v_{k+1}) = \lambda_{k+1}v_{k+1} = \lambda_{k+1}\left(\sum_{i=1}^k\mu_iv_i\right) = \sum_{i=1}^k\lambda_{k+1}\mu_iv_i
\]
Voegen we deze twee nu samen dan krijgen we het volgende.
\[
\sum_{i=1}^k\mu_i\lambda_iv_i = \sum_{i=1}^k\lambda_{k+1}\mu_iv_i
\]
\[
\sum_{i=1}^k \mu_i(\lambda_i-\lambda_{k+1})v_i = 0
\]
Omdat we weten dat alle $\lambda_i$ onderling verschillend zijn (gegeven) en dat alle $v_i$ lineair onafhankelijk zijn houdt dit in dat alle $\mu_i$ nul moeten zijn. Dit zou betekenen dat alle $v_i$ lineair onafhankelijk zijn en dat is in contradictie met de aanname van lineair afhankelijkheid.
\end{proof}

\subsection{Gevolg 5.20 p 191}
Zij $L$ een lineaire transformatie van de $n$-dimensionale vectorruimte $(\mathbb{R},V,+)$.

\subsubsection*{Te Bewijzen}
\begin{center}
$L$ heeft een enkelvoudig spectrum $\Rightarrow$ $L$ is diagonaliseerbaar.
\end{center}

\subsubsection*{Bewijs}
\begin{proof}
Neem voor elke eigenwaarde van $L$ de bijhorende eigenvector. Elk van die eigenvectoren zijn lineair onafhankelijk\footnote{Zie Stelling 5.18 190}. Omdat $L$ een enkelvoudig spectrum heeft zijn dit precies $n$ eigenvectoren. Deze vectoren zijn bijgevolg ook voortbrengend voor $V$ en een basis van $V$\footnote{Zie Stelling 3.41 p 109}. Nu voldoet $L$ aan de definitie van een diagonaliseerbare lineaire transformatie.
\end{proof}


\subsection{Lemma 5.22 p 192}
Zij $L$ een diagonaliseerbare lineaire transformatie van de eindig dimensionale vectorruimte $(\mathbb{R},V,+)$.

\subsubsection*{Te Bewijzen}
De karakteristieke veelterm van $L$ is volledig te ontbinden als product van eerstegraadsfactoren.

\subsubsection*{Bewijs}
\begin{proof}
Omdat $L$ diagonaliseerbaar is bestaat er een inverteerbare $P$ zodat $B$ een diagonaalmatrix is in $L = P^{-1}BP$ \footnote{Zie het bewijs van Stelling 5.7 p 181}. Omdat de karakteristieke veelter onafhankelijk is van de basis is deze voor $L_A$ en $L_B$ gelijk \footnote{Zie Gevolg 5.9 p 182}. De karakteristieke veelterm van $L_B$ is dus $\phi_L = \prod_{i=1}^n(X-\lambda_i)$ met $\lambda_i$ de waarden op de diagonaal van $B$ (dit zijn de eigenwaarden van $L$).  $\phi_L$ is bijgevolg een product van eerstegraadsvectoren.
\end{proof}


\subsection{Stelling 5.23 p 192}
Zij $(\mathbb{R},V,+)$ een vectorruimte van dimensie $n$ en zij $L$ een lineaire transformatie van $V$ waarvan de karakteristieke veelterm $\phi_L$ volledig te ontbinden valt als product van eerstegraadsfactoren. Zij $Spec(L) = \{\lambda_1,\lambda_2,...,\lambda_n\}$ het spectrum van $L$.

\subsubsection*{Te Bewijzen}
\begin{center}
$L$ is diagonaliseerbaar.
\end{center}
\[\Leftrightarrow\]
\[
\forall \text{ eigenwaarde } \lambda_i: d(\lambda_i) = m(\lambda_i)
\]

\subsubsection*{Bewijs}
\begin{proof}
Bewijs van een equivalentie.
\begin{itemize}
\item $\Rightarrow$\\
We weten dat er een basis $\beta$ van $V$ bestaat die volledig uit eigenvectoren van $L$ bestaat omdat $L$ diagonaliseerbaar is\footnote{Zie Definitie 5.6 p 181}. Rangschik de vectoren in $\beta$ nu volgens eigenwaarde. Per eigenwaarde zijn er dus $m(\lambda)$ vectoren. De vectoren die bij dezelfde eigenwaarde horen vormen een basis voor de eigenruimte van die eigenwaarde. Per eigenwaarde zijn er dus $d(\lambda)$ vectoren. Voor elke eigenwaarden geldt dus de volgende bewering.
\[
d(\lambda_i) = m(\lambda_i)
\]
Dit is gemakkelijk te zien als we matrixvoorstelling van $L$ ten opzichte van de eigenbasis opstellen.
\[
\begin{pmatrix}
\lambda_1 & 0 & \cdots & 0 & 0 & \cdots & 0 & \cdots & 0\\
0 & \lambda_1 & \cdots & 0 & 0 & \cdots & 0 & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots & \vdots & \ddots&\vdots & \ddots & \vdots\\
0 & 0 & \cdots & \lambda_1& 0 & \cdots   &0 & \cdots & 0\\
0 & 0 & \cdots & 0 & \lambda_2 & \cdots  &0 & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots & \vdots & \ddots & \vdots & \ddots&\vdots\\
0 & 0 & \cdots & 0 & 0 & \cdots & \lambda_2 & \cdots & 0\\
\vdots & \vdots & \ddots & \vdots & \vdots & \ddots & \vdots & \ddots&\vdots\\
0 & 0 & \cdots & 0 & 0 & \cdots & 0 & \cdots & \lambda_n\\
\end{pmatrix}
\]
Zoals we zien komt $\lambda_i$ precies $m(\lambda_i)$ keer voor en komen er met $\lambda_i$ precies $d(\lambda_i)$ eigenvectoren voor.

\item $\Leftarrow$\\
Voor elke eigenwaarde geldt $d(\lambda_i) = m(\lambda_i) = m_i$.
Omdat de karakteristieke veelterm volledig als product kan ontbonden worden in eerstegraadsfactoren en de karakteristieke veelterm van graag $n$ is (omdat de afmetingen van $L_A$ $n\times
n$ zijn) geldt de volgende bewering.
\[
\sum_{i=1}^km_i = n
\]
We kiezen nu voor elke eigenruimte $E_{\lambda_i}$ een basis $\beta_{E_{\lambda_i}} = \{v_{i,1},v_{i,2},...,v_{i,m_i}\}$. Zetten we nu al deze vectoren samen dan krijgen we de volgende verzameling.
\[
\beta = 
\{
v_{1,1},v_{1,2},...,v_{1,m_i},v_{2,1},v_{2,2},...,v_{2,m_i},v_{k,1},v_{k,2},...,v_{k,m_i}
\}
\]
Omdat $\sum_{i=1}^km_i = n$ geldt bevat deze verzameling $\beta$ precies $n$ elementen.
Als we nu kunnen bewijzen dat $\beta$ een basis vormt voor $V$ dan heeft $V$ dus een basis bestaande uit eigenvectoren en is $L$ bijgevolg diagonaliseerbaar. We zullen aantonen dat $\beta$ een vrij deel is van $V$, daaruit volgt dat $\beta$ een basis vormt voor $V$ \footnote{Zie Stelling 3.41 p 109}.\\\\
Stel nu dat er een lineaire combinatie van de vectoren in $\beta$ de nulvector oplevert.
\[
\sum_{i=1}^n\mu_{i,j}\beta_i = \vec{0}
\]
Omdat $\beta_i$ basissen zijn van de eigenruimten $E_{\lambda_i}$ geldt dat de volgende uitdrukking ook een vector is uit de eigenruimte $E_{\lambda_i}$ die bij eigenwaarde $i$ geldt. Dit betekent \emph{niet} per se dat $u_i$ eigenvectoren zijn, het kunnen namelijk ook nulvectoren zijn. Dit gaan we nu gebruiken.
\[
\sum_{j=1}^{m_i} \mu_{i,j}v_{i,j} = u_i
\]
De lineaire combinatie kan dus ook geschreven worden als volgt.
\[
\sum_{i=1}^ku_i = \vec{0}
\]
Alle $u_i$ in deze uitdrukking horen bij \emph{verschillende} eigenwaarden. Dit betekent dat de $u_i$ lineair onafhankelijk zijn \footnote{Zie stelling 5.18 p 190}. $u_i$ kunnen enkel lineair onafhankelijk zijn als alle $u_i$ nulvectoren zijn.
%Hier zit iets raar, lineair onafhankelijke nulvectoren? TODO
\[
u_1 = u_2 = ... = u_k
\]
Nu geldt voor elke $i$ dus het volgende.
\[
\sum_{j=1}^{m_i} \mu_{i,j}v_{i,j} = \vec{0}
\]
Omdat de $v_{i,j}$ basisvectoren van de eigenruimten $E_{\lambda_i}$ geldt dat alle $\mu_{i,j}$ nul moeten zijn.
\end{itemize}
\end{proof}


\subsection{Propositie 5.25 p 194}
Zij $L$ een lineaire transformatie van de eindig dimensionale vectorruimte $(\mathbb{R},V,+)$ waarvan de karakteristieke veelterm volledig te ontbinden valt als product van eerstegraadsfactoren.

\subsubsection*{Te Bewijzen}
\begin{enumerate}
\item De determinant van $L$ is gelijk aan het product van de eigenwaarden van $L$.
\item Het spoor van $L$ is gelijk aan de som van de eigenwaarden van $L$.
\end{enumerate}

\subsubsection*{Bewijs}
\begin{proof}
Bewijs door gefoefel.
\begin{enumerate}
\item
Zij $Spec(L) = \{\lambda_1,\lambda_2,...,\lambda_k\}$ het spectrum van $L$. Zij $m_i = m(\lambda)$ de algebra\"ische multipliciteit van eigenwaarde $i$. De karakteristieke veelterm van $L$ ziet er nu als volgt uit.
\[
\phi_L(X) = \prod_{i=1}^k(X-\lambda_i)^{m_i}
\]
Wanneer we dit product uitwerken ziet het er als volgt uit.% WAAROM
\[
X^n + (-1)(m_1\lambda_1 + m_2\lambda_2 + ... + m_3\lambda_3)X^{n-1} + (-1)^2(m_1\lambda_1 + m_2\lambda_2 + ... + m_3\lambda_3)^2X^{n-2} + ... + (-1)^n\prod_{i=1}^k\lambda_i^{m_i}
\]
Per defintie is de karakteristieke veelterm ook gelijk aan het volgende. In deze formule is $A$ de matrix van $L$ ten opzichte van eender welke basis van $V$.
\[
\phi_L(X) = det(X\mathbb{I}_n-A))
\]
%Hier gebeurt iets fishy, wtf?? Het is de eerste term van de bovenstaande uitdrukking, maar wut?
\[
(-s)^n\prod_{i=1}^k\lambda_i^{m_i} = \prod_{i=1}^k(-\lambda_i)^{m_i} = \phi_L(0) = det(-A) = (-1)^ndet(A) = (-1)^ndet(L) %Waar komt die laatste stap ineens vandaan??
\]
$det(L)$ is dus precies gelijk aan het product van alle eigenwaarden van $L$ waarbij elke eigenwaarde $\lambda_i$ $m_i$ keer geteld wordt.

\item
We beschouwen $\phi_L$ opnieuw op beide manieren.
\[
\phi_L = \prod_{i=1}^k(X-\lambda_i)^{m_i} \text{ en }\phi_L = det(X\mathbb{I}_n - A)
\]
In beide schrijfwijzen beschouwen we enkel de term bij co\"effici\"ent $X^{n-1}$. %waarom niet bij X^n??
\[
-\sum_{i=1}^km_i\lambda_i = -\sum_{k=1}^n(A)_{ii} = -Tr(A) = -Tr(L)
\]
Het spoor van $L$ is dus gelijk aan de som van alle eigenwaarden $\lambda_i$ waarin elke eigenwaarde $m_i$ keer geteld wordt.
\end{enumerate}
\end{proof}

\subsection{Stelling 5.28 p 203}
We zullen hiervoor geen bewijs geven. Dit moet je ook niet kunnen, maar zorg wel dat je goed begrijpt wat deze stelling inhoudt.
%Eventueel nog meer uitleg?

\subsection{Stelling 5.31 p 205}
Dit bewijzen we op dezelfde manier als Stelling 5.23 p 192
\subsection{Propositie 5.32 p 205}
Dit bewijzen we op dezelfde manier als Stelling 5.25 p 194

\subsection{Propositie 5.38 p 209}
Zij $X = (x_1 x_2 \cdots x_n)^T \in \mathbb{R}^n$ een kansvector en zijn $M$ een stochastische $n\times n$ matrix.
\subsubsection*{Te Bewijzen}
\[
M\cdot X \text{ is een kansvector.}
\]
\subsubsection*{Bewijs}
\begin{proof}
In een stochastische matrix en in een kansvector zijn alle elementen positief. Elk element van $M\cdot X$ is dus ook positief. Nu rest er ons nog te bewijzen dat de som van de elementen in $M \cdot X$ gelijk is aan $1$.
\end{proof}
\[
\sum_{i=1}^n(M\cdot X)_i = \sum_{i=1}^n\sum_{j=1}^nm_{ij}x_j= \sum_{j=1}^nx_j(\sum_{i=1}^nm_{ij})
\]
Let in bovenstaande vergelijking goed op de sommatie tekens. Voor de tweede gelijkheid wisselen we eerst de sommatietekens om. Dit mag omwille van de commutatitviteit van de optelling in $\mathbb{R}$. Daarna zonderen we de sommen van kolommen af. We weten dat die precies elke keer $1$ zijn omdat $M$ een stochastische matrix is.
\[
\sum_{j=1}^nx_j(\sum_{i=1}^nm_{ij}) = \sum_{j=1}^nx_j = 1
\]
De laatste gelijkheid geldt omdat $X$ een kansvector is, en de som van de elementen van $X$ bijgevolg precies $1$ is.

\subsection{Gevolg 5.39 p 209}
Zij alle $X_0$ een kansvector en $M$ een stochastische matrix.
\subsubsection*{Te Bewijzen}
Een markov keten bestaat uit een rij kansvectoren. Alle $X_i$ zijn dus kansvectoren.
\[
X0, X_1 = M\cdot X_0, X_2 = M^2\cdot X_0,...,X_n = M^n\cdot X_0
\]
\subsubsection*{Bewijs}
\begin{proof}
Bewijs door inductie op $n$.\\\\
\emph{Stap 1: (Basis stap)}\\
De bewering geldt voor $n=1$ \footnote{Zie Propositie 5.38 p 209}.\\\\
\emph{Stap 2: (Inductie stap}\\
Stel dat de bewering geldt voor een bepaalde $n=k$, dan bewijzen we nu dat daaruit volgt dat de bewering geldt voor $n=k+1$.
We moeten nu bewijzen dat de volgende vector een kansvector is.
\[
M^{k+1}\cdot X_0 = M^{k}\cdot M\cdot X_0
\]
Volgens de basisstap geldt dat $M\cdot X_0$ een kansvector is. Volgens de inductiehypothese geldt dat $M^{k}\cdot (M\cdot X_0)$ een kansvector is. Bijgevolg is $M^{k+1}\cdot X_0$ een kansvector.
\end{proof}





\section{Oefeningen 5.9}


\end{document}