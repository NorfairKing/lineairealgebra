\documentclass[lineaire_algebra_oplossingen.tex]{subfiles}
\begin{document}

\chapter{Theorie Hoofdstuk 6}

\section{Bewijzen uit de cursus}

\subsection{Opmerking 6.2 p 222}
Zij $(\mathbb{R},V,+)$ een reële vectorruimte.
\subsubsection*{Te Bewijzen}
Een inproduct is lineair in de tweede component.
\[
\forall w_1,w_2,v\in V, \forall \lambda_1,\lambda_2 \in \mathbb{R}:
\langle v,\lambda_1w_1+\lambda_2w_2\rangle = \lambda_1\langle v,w_1\rangle + \lambda_2\langle v,w_2\rangle
\]
\subsubsection*{Bewijs}
\begin{proof}
Kies drie willekeurige vectoren $w_1,w_2,v\in V$ en beschouw het volgende inproduct. Door lineariteit in de eerste component geldt de gelijkheid.
\[
\langle \lambda_1w_1+\lambda_2w_2,v\rangle = \lambda_1\langle w_1,v\rangle + \lambda_2\langle w_2,v\rangle
\]
Door symmetrie geldt dat deze twee leden ook gelijk zijn aan de volgende.
\[
\langle v,\lambda_1w_1+\lambda_2w_2\rangle = \lambda_1\langle v,w_1\rangle + \lambda_2\langle v,w_2\rangle
\]
\end{proof}

\subsection{Voorbeeld 6.6 p 224}
Definieer $\langle f,g\rangle$ als volgt.
\[
\langle f,g\rangle = \int_a^bf(x)\cdot g(x)dx
\]
Beschouw de vectorruimte $(\mathbb{R},C[a,b],+)$.
\subsubsection*{Te Bewijzen}
$\langle f,g\rangle$ is een inproduct.
\subsubsection*{Bewijs}
\begin{proof}
\begin{itemize}
\item $\langle f,g\rangle$ is lineair in de eerste component.
\[
\forall v_1,v_2,w\in V, \forall \lambda_1,\lambda_2 \in \mathbb{R}:
\langle \lambda_1v_1+\lambda_2v_2,w\rangle = \int_a^b\lambda_1 v_1(x)\cdot \lambda_2 v_2(x)dx
\]
\[
= \lambda_1\int_a^b v_1(x)\cdot \lambda_2\int_a^b v_2(x)dx = \lambda_1\langle v_1,w\rangle + \lambda_2\langle v_2,w\rangle
\]

\item $\langle f,g\rangle$ is symmetrisch.
\[
\forall v,w\in V: \langle v,w\rangle = \int_a^bv(x)\cdot w(x)dx= \int_a^bw(x)\cdot v(x)dx = \langle w,v\rangle
\]

\item $\langle f,g\rangle$ is positief.
\[
\forall v\in V: \langle v,v\rangle = \int_a^bv(x)\cdot v(x)dx \ge 0
\]
Dit is positief als want het is een kwadraat, namelijk van $\int_a^bv(x)dx$

\item $\langle f,g\rangle$ is definiet.
\[
\forall v\in V: \langle v,0\rangle = \vec{0}
\]
\[
\Leftrightarrow \int_a^bv(x)\vec{0}(x) dx = 0
\]
\end{itemize}
\end{proof}

\subsection{Stelling 6.11 p 228}
Zij $(\mathbb{R},V,+,\langle\cdot,\cdot\rangle)$ een inproductruimte met bijhorende norm.
\subsubsection*{Te Bewijzen}
\begin{enumerate}
\item 
\[
\forall \lambda\in\mathbb{R}, v\in V: \Vert\lambda v\Vert = \vert\lambda\vert\cdot \Vert v\Vert
\]

\item
\[
\forall v\in V: \Vert v\Vert\ge 0
\]

\item
\[
\forall v\in V: \Vert v\Vert\ge 0 \Leftrightarrow v=\vec{0}
\]
\end{enumerate}
\subsubsection*{Bewijs}
\begin{proof}
Direct bewijs.\\
Kies een willekeurige $v \in V$ en een $\lambda \in \mathbb{R}$
\begin{enumerate}
\item
\[
\Vert\lambda v\Vert = \sqrt{\langle \lambda v,\lambda v\rangle} = 
\sqrt{\lambda^2 \langle v,v\rangle}= \lambda\sqrt{\langle v,v\rangle} = \vert\lambda\vert\cdot \Vert v\Vert
\]

\item
\[
\Vert v\Vert= \sqrt{\langle v,v\rangle} \ge 0
\]
Een wortel is steeds positief en een inproduct is positief dus de wortel kan steeds getrokken worden \footnote{Zie Definitie 6.1 p 222}0.

\item
\item $\Rightarrow$\\
\[
\Vert v\Vert = 0 \Leftrightarrow \sqrt{\langle v,v\rangle}=0
\]
Dit kan enkel waar zijn als hetgeen onder de wortel ook nul is. Dit is zo omdat het inproduct definiet is \footnote{Zie Definitie 6.1 p 222}. 
\end{enumerate}
\end{proof}

\subsection{Stelling 6.14 p 229}
Zij $(\mathbb{R},V,+,\langle\cdot,\cdot\rangle)$ een inproductruimte.
\subsubsection*{Te Bewijzen}
\begin{enumerate}
\item 
\[
\forall v,w \in V: \vert\langle v,w\rangle\vert \le \Vert v\Vert\cdot \Vert w\Vert
\]
\item
\[
\forall v,w \in V: (\exists \lambda\in\mathbb{R} v = \lambda w) \Rightarrow \vert\langle v,w\rangle\vert = \Vert v\Vert\cdot \Vert w\Vert
\]
\end{enumerate}

\subsubsection*{Bewijs}
\begin{proof}
Bewijs door gevalsonderscheid.\\
Als $v$ en $w$ nulvectoren zijn gelden beide beweringen.
\[
\vert\langle \vec{0},\vec{0}\rangle\vert =0= \Vert \vec{0}\Vert\cdot \Vert \vec{0}\Vert
\]
We bewijzen de beweringen nu nog voor de andere gevallen.

\begin{enumerate}
\item
We weten dat voor willekeurige $v,w\in V$ geldt dat $\langle v + \lambda w,v+\lambda w\rangle \ge 0$. We werken dit uit.
\[
\langle v + \lambda w,v+\lambda w\rangle = \langle v,v+\lambda w \rangle + \lambda \langle w,v+\lambda w\rangle = \langle v,v \rangle + \lambda \langle v,w \rangle + \lambda \langle w,v\rangle + \lambda^2\langle w,w\rangle
\]
\[
\langle v,v \rangle + 2\lambda \langle v,w \rangle + \lambda^2\langle w,w\rangle \ge 0
\]
Het linker lid van deze ongelijkheid is een tweedegraads veelterm waarvan we weten dat ze altijd positief is. Bijgevolg is de discriminant ervan negatief.
\[
4\langle v,w\rangle^2-4\langle v,v\rangle\langle w,w\rangle \le 0
\]
\[
\langle v,w\rangle^2-\langle v,v\rangle\langle w,w\rangle \le 0
\]
\[
\langle v,w\rangle^2 \le \langle v,v\rangle\langle w,w\rangle 
\]
Voor diegenen die dit gefoefel vinden, u heeft gelijk.

\item
Als $w$ en $v$ lineair afhankelijk zijn dan bestaat er dus een $\lambda\in\mathbb{R}$ zodat $v=\lambda w$.
\[
|\langle \lambda w, w \rangle| = |\lambda  \langle w, w \rangle| = |\lambda\sqrt{\langle w, w \rangle^2}| = |\sqrt{\lambda^2\langle w, w \rangle^2}| = |\sqrt{\langle \lambda w, \lambda w \rangle \cdot \langle w, w \rangle }| \]
\[
= |\sqrt{\langle \lambda w, \lambda w \rangle} \sqrt{\langle w, w \rangle }| = |\sqrt{\langle v, v \rangle} \sqrt{\langle w, w \rangle }| = \Vert v\Vert\cdot \Vert w\Vert
\]
\end{enumerate}
\end{proof}

\subsection{Definitie 6.16 p 231}
Zij $(\mathbb{R},V,+,\langle\cdot,\cdot\rangle)$ een inproductruimte.
\subsubsection*{Te Bewijzen}
Voor elke $v,w\in V$ bestaat er een unieke hoek $\theta$ zodat de volgende gelijkheid geldt.
\[
\cos\theta = \frac{\langle v,w\rangle}{\Vert v\Vert\cdot \Vert w\Vert}
\]
\subsubsection*{Bewijs}
\begin{proof}
Direct bewijs\\
We weten dat voor elke $v,w\in V$ het volgende geldt.
\[
\vert\langle v,w\rangle\vert \le \Vert v\Vert\cdot \Vert w\Vert
\]
Hieruit volgt het volgende, zorg dat je dit begrijpt.
\[
-1 \le \frac{\langle v,w\rangle}{\Vert v\Vert\cdot \Vert w\Vert} \le 1
\]
We weten dat voor elk getal tussen $-1$ en $1$ er een hoek bestaat zodat de cosinus ervan dat getal is.
\end{proof}

\subsection{Stelling 6.19 p 232}
Zij $(\mathbb{R},V,+,\langle\cdot,\cdot\rangle)$ een inproductruimte met bijhorende norm $||\cdot ||$.
\subsubsection*{Te Bewijzen}
\[
\forall v,w \in V: \Vert v+w \Vert \le \Vert v\Vert + \Vert w\Vert
\]
\subsubsection*{Bewijs}
\begin{proof}
We weten dat $\Vert v+w \Vert$ en $\Vert v\Vert + \Vert w\Vert$ positief zijn. Bijgevolg is het voldoende te bewijzen dat $\Vert v+w \Vert^2 \le (\Vert v\Vert + \Vert w\Vert)^2$ geldt.
\[
 \Vert v+w \Vert^2 = \sqrt{\langle v+w,v+w\rangle}^2 = \langle v+w,v+w\rangle
\]
Let op, dit geldt enkel omdat $\Vert v+w \Vert$ positief is.
\[
= \langle v,v\rangle + \langle v,w\rangle + \langle w,v \rangle + \langle w,w \rangle =  \langle v,v\rangle + 2\langle v,w\rangle + \langle w,w \rangle
\]
Passen we nu drie keer de ongelijkheid van Cauchy-Schwartz toe dan verkrijgen we het volgende.
\[
\le \Vert v\Vert^2 + 2\Vert v\Vert \Vert w\Vert + \Vert w\Vert^2 = (\Vert v\Vert + \Vert w\Vert)^2
\]
\end{proof}

\subsection{Opmerking 6.20 p 232}
Zij $(\mathbb{C},V,+,\langle\cdot,\cdot\rangle)$ een hermetische ruimte.
\subsubsection*{Te Bewijzen}
\[
\forall v,w \in V: \Vert v+w \Vert \le \Vert v\Vert + \Vert w\Vert
\]
\subsubsection*{Bewijs}
\begin{proof}
Merk allereerst op dat het wel degelijk zin heeft om dit te bewijzen. Hoewel het hermetisch product beelden heeft in $\mathbb{C}$ heeft de norm nog steeds enkel beelden in $\mathbb{R}$.
We weten dat $\Vert v+w \Vert$ en $\Vert v\Vert + \Vert w\Vert$ positief zijn. Bijgevolg is het voldoende te bewijzen dat $\Vert v+w \Vert^2 \le (\Vert v\Vert + \Vert w\Vert)^2$ geldt.
\[
 \Vert v+w \Vert^2 = \sqrt{\langle v+w,v+w\rangle}^2 = \langle v+w,v+w\rangle = \langle v+w,v+w\rangle
\]
De lineairiteit in de tweede component geldt vanuit de definitie \footnote{Zie Definitie 6.3 p 223}. Over de eerste component weten we dat de toegevoegde lineariteit geldt \footnote{Zie Opmerking 6.4 p 223}. (In dit geval is de toegevoegde lineariteit voldoende omdat $\lambda = 1 \in \mathbb{R}$.)
\[
= \langle v+w,v\rangle + \langle v+w,w\rangle = \langle v,v\rangle + \langle v,w\rangle + \langle w,v \rangle + \langle w,w \rangle = \langle v,v\rangle + \langle v,w\rangle + \overline{\langle v,w \rangle} + \langle w,w \rangle
\]
We passen nu twee keer de ongelijkheid van Cauchy-Schwartz toe (op de eerste en de laatste term). Verder weten we ook nog dat de volgende gelijkheid geldt. We kunnen dezelfde ongelijkheid dus ook gebruiken voor de middelste termen.
\[
\langle v,w\rangle + \overline{\langle v,w \rangle} \le \vert\langle v,w\rangle\vert
\]
\[
\langle v,v\rangle + \langle v,w\rangle + \overline{\langle v,w \rangle} + \langle w,w \rangle \le \Vert v\Vert^2 + 2\Vert v\Vert \Vert w\Vert + \Vert w\Vert^2 = (\Vert v\Vert + \Vert w\Vert)^2
\]

\end{proof}

\subsection{Stelling 6.23 p 234}
Zij $(\mathbb{R},V,+,\langle\cdot,\cdot\rangle)$ een inproductruimte en zij $\alpha = \{v_1,v_2,...,v_k\}$ een deelverzameling van $k$ vectoren uit $\alpha$ zodat geen enkele $v_i$ de nulvector is en zodat voor twee verschillende $v_i,v_j (i\neq j)$ orthogonaal zijn ($\langle v_i,v_j \rangle = 0$). 
\subsubsection*{Te Bewijzen}
$\alpha$ is vrij.
\subsubsection*{Bewijs}
\begin{proof}
Bewijs uit het ongerijmde.\\
Stel dat de vectoren uit $\alpha$ lineaire afhankelijk zijn dan bestaan er $\lambda_i \in \mathbb{R}$ zodat de volgende gelijkheid opgaat.
\[
\sum_{i=1}^k \lambda_iv_i = \vec{0}
\]
Nu komt er gefoefel. Voor elke $i, j$ met $i\neq j$. (Noem $\mu_i =-\lambda_i $.)
De eerste gelijkheid geldt uit lineariteit van het inproduct \footnote{Zie opmerking 6.2 p 222 (ii)}.
\[
0 = \langle v_i, v_j \rangle = \langle v_i, \lambda_jv_j \rangle = \langle v_i, -\sum_{l=1, l\neq j}^k \lambda_lv_l \rangle = \sum_{l= 1}^k\langle v_i, \mu_lv_l\rangle = 0 + ... + 0 + \langle v_i,\mu_i v_i\rangle + 0 + ... + 0
\]
\[
= \mu_i \langle v_i,v_i\rangle = \mu_i \Vert v_i\Vert^2
\]
We weten dat $\Vert v_i\Vert^2$ strikt positief is aangezien $v_i \neq \vec{0}$. Hieruit volgt dat elke $\mu_i = -\lambda_i = 0$. Dit betekent dat $\alpha$ vrij is, en dat is in contradictie met de aanname.
\end{proof}

\subsection{Stelling 6.26 p 236}
Zij $(\mathbb{R}, V,+, \langle \cdot,\cdot \rangle)$ een euclidische ruimte en zij $\beta = \{v_1,v_2,...,v_n\}$ een \emph{orthonormale} basis van $V$. Zij $x,y\in \mathbb{R}^n$ twee willekeurige coördinaatvectoren ten opzichte van $\beta$. $x$ en $y$ zien er uit als volgt.
\[
x = (x_1,x_2,...,x_n) \text { en } y = (y_1,y_2,...,y_n)
\]
\subsubsection*{Te Bewijzen}
\[
\langle x,y\rangle = \sum_{i=1}^nx_iy_i
\]
\subsubsection*{Bewijs}
\begin{proof}
\[
\langle x,y\rangle = \left\langle \sum_{i=1}^nx_iv_i,\sum_{j=1}^ny_jv_j \right\rangle
\]
Het inproduct is lineaire in zowel de eerste als de tweede component\footnote{Zie Definitie 6.1 p 222 en opmerking 6.2 p 222.}.
\[
= \sum_{i=1}^nx_i \left\langle v_i,\sum_{j=1}^ny_jv_j \right\rangle
\]
\[
= \sum_{i=1}^nx_i \sum_{j=1}^n y_j \left\langle v_i,v_j \right\rangle
\]
We weten dat voor elke $i$ en $j$ zodat $i\neq j$ geldt dat $\langle v_j, v_i \rangle = 0$. Bovendien, als $i=j$ dan geldt $\langle v_j, v_i \rangle = \langle v_i, v_i \rangle = \Vert v\Vert^2$.
Omdat $\beta$ een orthonormale basis is is $i$-de component van de basisvector precies gelijk aan de norm van die vector en ook gelijk aan $1$. 
\[
= \sum_{i=1}^nx_i  (y_10+...+y_{i-1}0+y_i\Vert v_i\Vert^2+y_{i+1}0+...+y_n)
\]
\[
= \sum_{i=1}^n x_i y_i \Vert v_i\Vert^2
\]
\[
= \sum_{i=1}^nx_i y_j
\]
\end{proof}

\subsection{Stelling 6.27 p 238}
Zij $(\mathbb{R}, V,+, \langle \cdot,\cdot \rangle)$ een euclidische ruimte en zij $\beta = \{v_1,v_2,...,v_n\}$ een willekeurige basis van $V$.

\subsubsection*{Te Bewijzen}
$\beta$ kan omgevormd worden tot een orthonormale basis $\gamma = \{u_1,u_2,...,u_n\}$.

\subsubsection*{Bewijs}
\begin{proof}
Bewijs door constructie.\\
We zullen eenvoudigweg het algoritme beschrijven en de correctheid ervan bewijzen.\\\\
We normeren de eerste vector $v_1$ door ze te delen door zijn norm.
\[
u_1 = \frac{1}{\Vert v_1\Vert}v_1
\]
We construeren nu $v_2'$ zodat ze loodrecht staat op $u_1$.
\[
v_2' = v_2 - \langle v_2,v_1\rangle u_1
\]
$v_2'$ is zeker niet nul omdat $u_1$ ofwel niet op dezelfde rechte ligt als $v_2$, ofwel is $v_2' = \vec{0}$. $v_2'$ staat nu wel degelijk loodrecht op $u_1$.
\[
\langle v_2',u_1\rangle = \langle v_2 - \langle v_2,v_1\rangle u_1,u_1\rangle = \langle v_2,u_1\rangle - \langle\langle v_2,v_1\rangle u_1,u_1\rangle
\]
\[
= \langle v_2,u_1\rangle - \langle v_2,v_1\rangle \langle u_1,u_1\rangle = \langle v_2,u_1\rangle - \langle v_2,v_1\rangle \Vert u_1\Vert = \langle v_2,u_1\rangle - \langle v_2,v_1\rangle = 0
\]
Merk op dat $u_1$ echt genormeerd moet zijn, opdat dit zou werken. Tenslotte normeren we $v_2'$ nog om $u_2$ te bekomen.
\[
u_2 = \frac{1}{\Vert v_2'\Vert}v_2' = \frac{1}{\Vert v_2 - \langle v_2,v_1\rangle u_1\Vert}(v_2 - \langle v_2,v_1\rangle u_1)
\]
Dit proces kunnen we verder zetten. We beschrijven nu de stappen die je moet ondernemen in iteratie $k+1$.
We construeren $v_{k+1}'$ als volgt.
\[
v_{k+1}' = v_{k+1} - \langle v_{k+1},u_1 \rangle u_1 - \langle v_{k+1},u_2 \rangle u_2 - ... - \langle v_{k+1},u_k \rangle u_k
= v_{k+1} - \sum_{i=1}^k \langle v_{k+1},u_i \rangle
\]
Beschouw de sommatie in deze formule als \'e\'en vector. Nu is de redenering om te besluiten dat $v_{k+1}'$ dezelfde als die bij $v_2'$.
$v_{k+1}'$ is staat inderdaad loodrecht op alle vectoren $v_i$ met $i\le k$.

\[
\langle v_{k+1}',u_i\rangle  = \left\langle v_{k+1} - \sum_{j=1}^k \langle v_{k+1},u_j \rangle u_j,u_i\right\rangle
=
\langle v_{k+1},u_i\rangle -
\left\langle \sum_{j=1}^k \langle v_{k+1},u_j \rangle u_j,u_i\right\rangle
\]
\[
=
\langle v_{k+1},u_i\rangle -
\sum_{j=1}^k
\left\langle  \langle v_{k+1},u_j \rangle u_j,u_i\right\rangle
=
\langle v_{k+1},u_i\rangle -
\sum_{j=1}^k
\langle v_{k+1},u_j \rangle
\left\langle u_j,u_i\right\rangle
\]
In de laatste som is elke term behalve de term waarbij $i=j$ geldt nul.
\[
\langle v_{k+1},u_i\rangle -
\langle v_{k+1},u_i\rangle
=0
\]
We kunnen nu $v_{k+1}'$ ook normeren door te delen door de norm.
\[
u_{k+1} = \frac{1}{\Vert v_{k+1}'\Vert}v_{k+1}'
\]
Omdat $V$ eindig dimensionaal is zal dit algoritme zeker stoppen. Het resultaat is $\{u_1,u_2,...,u_n\}$, een orthonormale basis.
\end{proof}

\subsection{Definitie 6.30 p 240}
Zij $(\mathbb{R}, V,+, \langle \cdot,\cdot \rangle)$ een euclidische ruimte en zij $U$ een deelruimte van $V$.
\subsubsection*{Te Bewijzen}
\[
U^\bot \text{ is een deelruimte van } V
\]
\subsubsection*{Bewijs}
\begin{proof}
Bewijs volgens Stelling 3.11 p 94.
\begin{itemize}
\item $\vec{0} \in U^\bot$\\
Dit is niet intu\"itief, maar de nulvector staat loodrecht op elke vector.
\[
\forall u \in U: \langle u, \vec{0} \rangle
\]
\end{itemize}

\item De lineaire combinatie in $U^T$ is intern.
\[
\forall v_1,v_2\in U^\bot, \lambda_1,\lambda_2 \in \mathbb{R} : \lambda_1v_1 + \lambda_2v_2 \in U^T
\]
Voor elke vector $u'$ in $U^\bot$ geldt het volgende, dus ook voor willekeurige $v_1,v_2\in U^T$.
\[
\forall u\in U^\bot: \langle u',u\rangle = 0
\]
Nu moeten we aantonen dat dit ook voor de lineaire combinatie geldt. Kies $u$, een willekeurige vector in $U$.
\[
\langle v_1,u\rangle = 0 \text{ en } \langle v_2,u\rangle = 0
\]
\[
\Rightarrow
\lambda_1\langle v_1,u\rangle = 0 \text{ en } \lambda_2\langle v_2,u\rangle = 0
\]
\[
\Rightarrow
\lambda_1\langle v_1,u\rangle + \lambda_2\langle v_2,u\rangle = 0
\]
\[
\Rightarrow
\langle \lambda_1v_1,u\rangle + \langle \lambda_2v_2,u\rangle = 0
\]
\[
\Rightarrow
\langle \lambda_1v_1 + \lambda_2v_2,u\rangle = 0
\]
De laatste twee implicaties gelden volgens de lineariteit van het inproduct \footnote{Zie Definitie 6.1 en opmerking 6.2 p 222}.
\end{proof}

\subsection{Stelling 6.31 p 240}
Zij $(\mathbb{R}, V,+, \langle \cdot,\cdot \rangle)$ een euclidische ruimte en zij $U$ een deelruimte van $V$.

\subsubsection*{Te Bewijzen}
\begin{enumerate}
\item De som van $U$ en $U^T$ is een directe som.
\[U + U^\bot = U \oplus U^\bot\]
\item
\[U\oplus U^\bot = V\]
\item
\[dimU + dimU^\bot = dim V\]
\item
\[(U^\bot)^\bot = U\]
\end{enumerate}

\subsubsection*{Bewijs}
\begin{proof}
Direct bewijs.
\begin{enumerate}
\item
We moeten hier bewijzen dat de doorsnede van $U$ en $U^\bot$ enkel de nulvector is.

Kies een willekeurige vector $v$ in $U$ die ook in $U^\bot$ zit. Omdat $v$ in $U^\bot$ zit staat hij loodrecht op elke vector in $U$, dus op zichzelf.
\[
\langle v,v \rangle = 0
\]
Omdat het inproduct definiet is geldt $v=\vec{0}$ \footnote{Zie Definitie 6.1 p 222}.

\item
Omdat we het vorige al bewezen hebben moeten we enkel nog bewijzen dat elke vector $v\in V$ geschreven kan worden als de som van een vector $u \in U$ en een vector $u'\in U^\bot$.\\
Neem een basis van $U$ en orthonormaliseer deze tot een basis $\beta_U = \{u_1,u_2,...,u_k\}$
Kies een willekeurige vector $v\in V$. $v$ moet geschreven kunnen worden als een lineaire combinatie van de basis $\beta_U$ van $u$ en een vector $u' \in U^\bot$.
\[
v = u + u' = \sum_{i=1}^k\langle v,u_i\rangle u_i + u'
\]
Nu moeten we nog bewijzen dat $u'$ orthogonaal is met alle basisvectoren $u_i$ van $U$. Voor elke $i$ moet dus het volgende gelden.
\[
\langle v',u_i \rangle = \langle v,u_i\rangle - \langle v,u_i\rangle\langle u_i,u_i\rangle = 0
\]
Omdat $u_i$ genormeerde vectoren zijn geldt bovenstaande gelijkheid.\\
Dit houdt precies in dat $v$ geschreven kan worden op precies \'e\'en manier als een som van een vector in $U$ en een vector in $U'$.

\item
Omdat $U \oplus U' = V$ geldt $dimU+dimU^\bot = dimV$ ook \footnote{Zie Stelling 3.52 p 115}.

\item
Uit de definitie van het orthogonaal complement volgt dat $U \subset (U^\bot)^\bot$ geldt \footnote{Zie Definitite 6.30 p 240}. $U$ en $(U^\bot)^\bot$ hebben dezelfde dimensie, en zijn daarom gelijk. 
\end{enumerate}
\end{proof}

\subsection{Voorbeeld 6.32 p 241}
\subsubsection*{Te bewijzen}
In $\mathbb{R}^3$ is het orthogonaal complement van de $x-as$ het volledige $yz$-vlak.
\subsubsection*{Bewijs}
\begin{proof}
Zij $\{e_1,e_2,e_3\}$ de standaardbasis van $\mathbb{R}^3$. Elke vector op de $x$-as is een veelvoud van $e_1$ en elke vector op het $xy$-vlak kan geschreven worden als een lineaire combinatie van $e_2$ en $e_3$.
\[
\lambda_1e_1 \in x \text{ en }\lambda_2e_2 + \lambda_3e_3 = v\in yz
\]
We bewijzen nu dat elke vector op het $yz$ vlak loodrecht staat op elke vector op de $x$ as.
Inderdaad, kies willekeurige $\lambda_1,\lambda_2,\lambda_3 \in \mathbb{R}$.
\[
\langle \lambda_1e_1 , \lambda_2e_2 + \lambda_3e_3 \rangle = \langle \lambda_1e_1 , \lambda_2e_2
\rangle + \langle \lambda_1e_1 , \lambda_3e_3 \rangle = \lambda_1\lambda_2\langle e_1 , e_2
\rangle + \lambda_1\lambda_3\langle e_1 , e_3 \rangle = 0
\]
De eerste twee gelijkheden gelden omwille van de lineariteit van het inproduct \footnote{Zie Definitie 6.1 p 222}.
De laatste gelijkheid geldt omdat $e_1,e_2,e_3$ orthonormaal zijn.
\end{proof}

\subsection{Stelling 6.34 p 242}
Zij $U$ een eindig dimensionale deelruimte van de euclidische ruimte $(\mathbb{R}, V,+, \langle \cdot,\cdot \rangle)$.
\subsubsection*{Te Bewijzen}
Voor elke $v\in V$ geldt dat $pr_U(v)$ de vector in $U$ op minimale afstand van $v$ is.

\subsubsection*{Bewijs}
\begin{proof}
We moeten bewijzen dat voor elke andere $u\in U$ geldt dat de afstand van $v$ tot $u$ groter is dan of gelijk is aan de afstand van $v$ tot $pr_U(v)$. Omdat afstanden altijd positief zijn mogen we het ook bewijzen met kwadraten.
\[
\Vert v- u \Vert^2 = \langle (v-u) , (v-u)\rangle = \langle (v-pr_U(v)+pr_U(v)-u) , (v-pr_U(v)+pr_U(v)-u)\rangle
\]
\[
= \langle (v-pr_U(v), (v-pr_U(v)+pr_U(v)-u)\rangle + \langle (pr_U(v)-u) , (v-pr_U(v)+pr_U(v)-u)\rangle
\]
\[
= \langle (v-pr_U(v), (v-pr_U(v))\rangle + \langle (v-pr_U(v), (pr_U(v)-u)\rangle\]\[ +\langle (pr_U(v)-u) , (v-pr_U(v))\rangle + \langle (pr_U(v)-u) , (pr_U(v)-u)\rangle
\]
\[
= \langle (v-pr_U(v), (v-pr_U(v))\rangle + +2\langle (pr_U(v)-u) , (v-pr_U(v))\rangle + \langle (pr_U(v)-u) , (pr_U(v)-u)\rangle
\]
De middelste term in bovenstaande vergelijking geldt omdat $pr_U-u$ een vector uit $U$ is en omdat $v-pr_U(v)$ een vector uit $U^\bot$ is.
\[
 = \langle (v-pr_U(v), (v-pr_U(v))\rangle + \langle (pr_U(v)-u) , (pr_U(v)-u)\rangle
\]
\[
= \Vert v-pr_U(v) \Vert^2 + \langle (pr_U(v)-u) , (pr_U(v)-u)\rangle
\]
Onderstaande ongelijkheid geldt omdat het inproduct positief is \footnote{Zie Definitie 6.1 p 222}.
\[
\ge \Vert v-pr_U(v) \Vert^2
\]
\end{proof}

\subsection{Lemma 3.36 p 243}
Zij $(\mathbb{R},\mathbb{R}^n,+,\langle \cdot , \cdot \rangle)$ de standaard euclidische ruimte en $L_A : \mathbb{R}^n \rightarrow \mathbb{R}^n: X \mapsto A \cdot X$ een lineaire transformatie met een symmetrische matrix $A$.
\subsubsection*{Te Bewijzen}
$A$ heeft uitsluitend re\"ele eigenwaarden.
\subsubsection*{Bewijs}
\begin{proof}
Bewijs door gefoefel.\\
We vertrekken algemener van de standaard unitaire ruimte $(\mathbb{C},\mathbb{C}^n,+,\langle \cdot , \cdot \rangle)$ met standaard hermetisch product $\langle X,Y\rangle = \overline{X}^T \cdot Y$. De lineaire transformatie met matrix $A$ kan ook opgevat worden als een lineaire transformatie $\mathbb{C}^n,\mathbb{C}^n$. Omwille van de hoofdstelling van de algebra houdt dit in dat $\phi_A$ volledig kan ontbonden worden als product van eerstegraadsfactoren in $\mathbb{C}$ \footnote{Zie Stelling 5.28 p 203.}.
We zullen nu bewijzen dat elk nulpunt van $\phi_A$ in $\mathbb{C}$ ook in $\mathbb{R}$ zit.
We beginnen met $\lambda \langle X , X \rangle$ en proberen aan te tonen dat dit gelijk is aan $\overline{\lambda} \langle X , X \rangle$. Dit is gefoefel maar het werkt. 
\[
\lambda \langle X , X \rangle = \langle X , \lambda X \rangle
\]
Bovenstaande vergelijking geldt omwille van de lineariteit van het hermetisch product \footnote{Zie Definitie 6.3 p 223.}.
Onderstaande gelijkheid geldt omdat $\lambda$ een eigenwaarde is van $A$.
\[
= \langle X , A \cdot X \rangle = \overline{X}^T \cdot A \cdot X
\]
$A = \overline{A}^T$ is gelijk aan zijn complement omdat $A \in \mathbb{R}^{n\times n}$ re\"eel en symmetrisch is. Voor de tweede gelijkheid foefelen we wat met getransponeerde matrices \footnote{Zie Eigenschap 1.22 p 32 bovenaan (c).}
\[
= \overline{X}^T \cdot \overline{A}^T \cdot X = (\overline{A \cdot X})^T \cdot X
\]
Opnieuw gebruiken we de definitie van het hermetisch product en het feit dat $\lambda$ een eigenwaarde is van $A$.
\[
= \langle  A \cdot X ,X \rangle =  \langle \lambda X , X \rangle = \overline{\lambda} \langle X , X \rangle
\]
Omdat $\lambda$ gelijk is aan zijn complement zit $\lambda$ in $\mathbb{R}$.
\end{proof}

\subsection{Lemma 6.38 p 244}
Zij $(\mathbb{R},\mathbb{R}^n,+,\langle \cdot , \cdot \rangle)$ de standaard euclidische ruimte en $L_A : \mathbb{R}^n \rightarrow \mathbb{R}^n: X \mapsto A \cdot X$ een lineaire transformatie met een symmetrische matrix $A$.
\subsubsection*{Te Bewijzen}
Eigenvectoren bij verschillende eigenwaarden van $A$ zijn orthogonaal.
\subsubsection*{Bewijs}
\begin{proof}
Direct bewijs door gefoefel.\\
De eigenwaarden van $A$ zijn zeker re\"eel \footnote{Zie Lemma 6.36 p 243.}. Kies nu twee eigenwaarden $\lambda_1 \neq \lambda_2$. 
\[
\lambda_2 \langle X_1 , X_2 \rangle = \langle X_1, \lambda_2X_2 \rangle=\langle X_1, A\cdot X_2 \rangle
\]
We gebruiken de lineariteit van het inproduct\footnote{Zie Definitie 6.1 p 222} en het feit dat $\lambda_2$ een eigenwaarde is van $A$ met eigenvector $X_2$. Vervolgens gebruiken we de definitie van het standaard inproduct \footnote{Zie Voorbeeld 6.6 p 224.} en een eigenschap van getransponeerde matrices \footnote{Zie Eigenschap 1.22 p 32 bovenaan (c).}.
\[
X_1^T \cdot A\cdot X_2 = (A \cdot X_1)^T \cdot X_2
\]
Nu zetten we dit weer binnen het inproduct en gebruiken we het feit dat $\lambda_1$ een eigenwaarde is van $A$ met eigenvector $X_1$.
\[
\langle A \cdot X_1 , X_2 \rangle = \langle \lambda_1 \cdot X_1 , X_2 \rangle = \lambda_1 \langle X_1 , X_2 \rangle 
\]
Dit betekent dat $(\lambda 2-\lambda_1)\langle X_1 , X_2 \rangle =0$. Omdat $\lambda_1$ en $\lambda_2$ verschillend zijn moet $\langle X_1 , X_2 \rangle$ nul zijn. Dit betekent precies dat $X_1$ en $X_2$ loodrecht staan $X_1 \bot X_2$.
\end{proof}

\subsection{Lemma 6.40 p 245}
Zij $(\mathbb{R},\mathbb{R}^n,+,\langle \cdot , \cdot \rangle)$ de standaard euclidische ruimte en $L_A : \mathbb{R}^n \rightarrow \mathbb{R}^n: X \mapsto A \cdot X$ een lineaire transformatie met een symmetrische matrix $A$.
\subsubsection*{Te Bewijzen}
Voor elke eigenwaarde van $A$ is de meetkundige multipliciteit $d(\lambda)$ gelijk aan de algebra\"ische multipliciteit $m(\lambda)$.
\[
\forall\lambda: d(\lambda) = m(\lambda)
\]
\subsubsection*{Bewijs}
\begin{proof}
Zij $\lambda$ een willekeurige eigenwaarde van $A$. De bijhorende eigenruimte $E_\lambda$ heeft dimensie $d(\lambda)$. We willen aantonen dat $d(\lambda)$ gelijk is aan $m(\lambda)$.\\
We weten dat $\mathbb{R}^n = E_\lambda \oplus E_\lambda^\bot$ geldt \footnote{Zie Stelling 6.31 p 240}. We kunnen dus een basis van $E_\lambda$ kiezen (van eigenvectoren van $A$ bij $\lambda$) en deze uitbreiden tot een basis van $\mathbb{R}^n$ met een basis van $E_\lambda^\bot$.
Kies een vector $Y\in E_\lambda^\bot$. $A\cdot Y$ behoort dan ook tot $E_\lambda^\bot$ want voor elke $X \in E_\lambda$
\[
\langle X, A\cdot Y\rangle \overset{A \text{ is symmetrisch}}{=} \langle A\cdot X, Y \rangle = \lambda \langle X,Y \rangle = 0
\]
We kunnen nu $L_A$ beperken tot $E_\lambda^\bot$. Deze beperking heeft dan $B$ als matrixvoorstelling. De matrixvoorstelling van $L_A$ ten opzichte van $\beta$ is dan een matrix van de volgende vorm.
\[
\begin{pmatrix}
\lambda\mathbb{I}_{d(\lambda} & 0\\
0 & B
\end{pmatrix}
\]
$B$ kan nu niet meer de eigenwaarde $\lambda$ hebben. Dit houdt ook in dat $d(\lambda) = m(\lambda)$.
\end{proof}

\subsection{Stelling 6.42 p 246}
Zij $(\mathbb{R},\mathbb{R}^n,+,\langle \cdot , \cdot \rangle)$ de standaard euclidische ruimte en $L_A : \mathbb{R}^n \rightarrow \mathbb{R}^n: X \mapsto A \cdot X$ een lineaire transformatie met een symmetrische matrix $A$.

\subsubsection*{Te Bewijzen}
Er bestaat een orthonormale basis voor $\mathbb{R}^n$ bestaande uit eigenvectoren van $A$.
Dit houdt in dat $A$ gelijkvormig is met een diagonaalmatrix.
De gelijkvormigheid kan gerealiseerd worden door een basisverandering van de orthonormale standaardbasis naar een orthonormale basis.
We noemen $A$ dan orthogonaal diagonaliseerbaar.

\subsubsection*{Bewijs}
\begin{proof}
Zij $\lambda_1,\lambda_2,...,\lambda_n$ de eigenwaarden van $A$ met bijhorende eigenvectoren $v_1,v_2,...,v_n$ en eigenruimten $E_{\lambda_1},E_{\lambda_2},...,E_{\lambda_n}$.
We kiezen nu voor elke eigenruimte $E_{\lambda_i}$ een basis en orthonormaliseren deze \footnote{Zie Stelling 6.27 p 238.} tot een orthonormale basis $\beta_i$.
Omdat $L$ een lineaire transformatie is met een symmetrische matrix, is voor elke eigenwaarde de meetkundige multipliciteit gelijk aan de algebra\"ische \footnote{Zie Lemma 6.40 p 245.}. Dit houdt in dat $L$ diagonaliseerbaar is \footnote{Zie Stelling 5.23 p 192.}. Bijgevolg is de (disjuncte unie van de basissen van $E_{\lambda_i}$ een basis van $V$.
\[
\biguplus_{i=1}^k \beta_i
\]
Omdat alle $\beta_i$ orthonormale basissen zijn en ze lineair onafhankelijk \footnote{Zie Stelling 5.18 p 190.} zijn vormt de disjunctie unie ervan ook een orthonormale basis.
\end{proof}


\subsection{Stelling 6.43 p 246}
\subsubsection*{Te Bewijzen}
\subsubsection*{Bewijs}
\begin{proof}

\end{proof}


\subsection{Stelling 6.46 p 249}
Zij $(\mathbb{R},V,+,\langle \cdot , \cdot \rangle)$ een euclidische ruimte en $A\in \mathbb{R}^{n\times n}$ een vierkante matrix.
\subsubsection*{Te Bewijzen}
\begin{center}
De kolommen van $A$ vormen een orthonormale basis van $\mathbb{R}^n$
\end{center}
\[\Leftrightarrow\]
\[A^T \cdot A = \mathbb{I}_n\]
\[\Leftrightarrow\]
\[A^T =A^{-1}\]
\[\Leftrightarrow\]
\[A \cdot A^T = \mathbb{I}_n\]
\[\Leftrightarrow\]
\begin{center}
De rijen van $A$ vormen een orthonormale basis van $\mathbb{R}^n$
\end{center}
\subsubsection*{Bewijs}
\begin{proof}
Bewijs door circulaire implicaties.
\begin{itemize}
\item
De kolommen van $A$ vormen een orthonormale basis van $\mathbb{R}^n \Rightarrow A^T \cdot A = \mathbb{I}_n$.\\
We beschrijven element $(A^T \cdot A)_{ij}$. Noem $a_i$ de $i$-de kolom van $A$.
\[
(A^T \cdot A)_{ij} = \langle a_i , a_j\rangle
\]
Dit inproduct is gelijk aan nul als $i\neq j$, precies omdat de kolommen van $A$ een orthonormale basis vormen van $\mathbb{R}^n$.
Als $i =j$ (op de diagonaal), dan is dit inproduct $\langle a_i , a_i \rangle = \Vert a_i\Vert^2 = 1$. De matrix $(A^T \cdot A)$ is dus gelijk aan de eenheidsmatrix.

\item
$A^T \cdot A = \mathbb{I}_n \Rrightarrow A^T =A^{-1}$\\
We vermenigvuldigen eenvoudigweg beide kanten rechts met $A^{-1}$.

\item
$A^T =A^{-1} \Rightarrow A \cdot A^T = \mathbb{I}_n$\\
We vermenigvuldigen eenvoudigweg beide kanten links met $A$.

\item
$A \cdot A^T = \mathbb{I}_n \Rightarrow $ De rijen van $A$ vormen een orthonormale basis.\\
Deze redenering is precies gelijk aan de redenering in het eerste deel van dit bewijs.

\item
De rijen van $A$ vormen een orthonormale basis. $\Rightarrow$ De kolommen van $A$ vormen een orthonormale basis.\\
Omdat de rijen een basis vormen vormen de kolommen ook een basis \footnote{Zie Stelling 4.42 p 161 (d-e) en Stelling 3.41 p 109.}. Nu rest er ons nog te bewijzen dat de rijen van $A$ ook orthogonaal en genormeerd zijn.
TODO

\end{itemize}
\end{proof}

\subsection{Stelling 6.48 p 246}
Zij $A\in \mathbb{R}^{n\times n}$ een symmetrische matrix.
\subsubsection*{Te Bewijzen}
Er bestaat een orthogonale matrix $P\in \mathbb{R}^{n\times n}$ zodat $P^{-1}AP = P^TAP$ een diagonaalmatrix is.
\subsubsection*{Bewijs}
\begin{proof}

\end{proof}

\subsection{Stelling 6.49 p 249}
Zij $A\in \mathbb{R}^{n\times n}$ een orthogonale matrix.
\subsubsection*{Te Bewijzen}
\begin{enumerate}
\item De determinant van $A$ is $1$ of $-1$.
\item Elke (re\"eele eigenwaarde van $A$ is $1$ of $-1$.
\end{enumerate}
\subsubsection*{Bewijs}
\begin{proof}
Direct bewijs.
\begin{enumerate}
\item
\[
A^{-1} = A^T
\]
\[
det(A^{-1}) = det(A^T)
\]
\[
\frac{1}{det(A)} = det(A)
\]
\[
1 = det(A)^2
\]
Dit kan alleen kloppen als de derminant van $A$ $1$ of $-1$ is.
\item
Let op: gefoefel! Dit is geen  moeilijk bewijs als u weet hoe eraan te beginnen. Een manier om erop te komen is door in te zien dat we moeten bewijzen dat $\lambda^2$ gelijk moet zijn aan $1$.\\
Zij $\lambda$ een eigenwaarde van $A$ met eigenvector $v$.
\[
\lambda^2\langle v,v\rangle = \langle \lambda v,\lambda v\rangle = \langle A v,A v\rangle = v^TA^T A v = v^TA^{-1} A v = v^Tv = \langle v, v \rangle
\]
Dit houdt precies in dat $\lambda^2 = 1$ geldt. $\lambda$ moet dus $1$ of $-1$ zijn. Omdat deze bewering geldt voor een willekeurige eigenwaarde $\lambda$ van $A$ geldt ze voor elke eigenwaarde van $A$.

\end{enumerate}
\end{proof}

\subsection{Stelling 6.50 p 250}
Zij $(\mathbb{R},\mathbb{R}^n,+,\langle \cdot , \cdot \rangle)$ de standaard euclidische ruimte   en $L_A : \mathbb{R}^n \rightarrow \mathbb{R}^n: X \mapsto A \cdot X$ een lineaire transformatie met een orthogonale matrix $A$. Zij $u_1,...,u_n$ een orthonormale basis voor $\mathbb{R}^n$.

\subsubsection*{Te Bewijzen}
\begin{enumerate}
\item $\forall X \in \mathbb{R}^n: \Vert A \cdot X \Vert = \Vert X \Vert$

\item $\forall X,Y \in \mathbb{R}^n: d(A\cdot X,A\cdot Y) = d(X,Y)$

\item $L_A$ is een isomorfisme.

\item $L_A(u_1),...;L_A(u_n)$ is een orthonormale basis voor $\mathbb{R}^n$.

\end{enumerate}

\subsubsection*{Bewijs}
\begin{proof}
Direct bewijs.\\
Het bewijs in de cursus is wel juist, maar als u nog steeds niet overtuigd bent is hier een gedetailleerder bewijs.
\begin{enumerate}
\item
\[
\Vert A \cdot X \Vert = \langle A\cdot X, A\cdot X\rangle = X^TA^TAX =  X^TA^{-1}AX =\langle X,X\rangle = \Vert X\Vert
\]

\item
We spreken over de standaard euclidische ruimte, dus kunnen we definitie van het standaard euclidisch product gebruiken \footnote{Zie Voorbeeld 6.6 p 224.}.
\[
\Vert A\cdot X - A\cdot Y \Vert = \sqrt{\langle  A\cdot X - A\cdot Y ,  A\cdot X - A\cdot Y \rangle} = \sqrt{(A\cdot X - A\cdot Y)^T\cdot(A\cdot X - A\cdot Y)}
\]
\[
= \sqrt{((A\cdot X)^T - (A\cdot Y)^T)\cdot(A\cdot X - A\cdot Y)} = \sqrt{(X^T\cdot A - Y^T\cdot A^T)\cdot(A\cdot X - A\cdot Y)}
\]
\[
= \sqrt{((A\cdot X)^T - (A\cdot Y)^T)\cdot(A\cdot X - A\cdot Y)}
\]
\[ 
= \sqrt{(X^T\cdot A^T - Y^T\cdot A^T)\cdot(A\cdot X) - (X^T\cdot A^T - Y^T\cdot A^T)\cdot (A\cdot Y)}
\]
\[ 
= \sqrt{(X^T\cdot A^T)\cdot(A\cdot X) - (Y^T\cdot A^T)\cdot(A\cdot X) - (X^T\cdot A^T)\cdot (A\cdot Y) + (Y^T\cdot A^T)\cdot (A\cdot Y)}
\]
\[ 
= \sqrt{(X^T\cdot A^{-1})\cdot(A\cdot X) - (Y^T\cdot A^{-1})\cdot(A\cdot X) - (X^T\cdot A^{-1})\cdot (A\cdot Y) + (Y^T\cdot A^{-1})\cdot (A\cdot Y)}
\]
\[ 
= \sqrt{(X^T\cdot X) - (Y^T\cdot X) - (X^T\cdot Y) + (Y^T\cdot Y)}
\]
\[ 
= \sqrt{(X^T- Y^T)\cdot X - (X^T - Y^T) \cdot Y}
= \sqrt{(X^T- Y^T)\cdot (X - Y)}
\]
\[ 
= \sqrt{(X- Y)^T\cdot (X - Y)}
= \sqrt{\langle X-Y , X-Y \rangle}
= \Vert X-Y\Vert
\]

\item
Om een isomorfisme te zijn moet $L_A$ bijectief en lineair zijn \footnote{Zie Definitie 4.15 p 145.}. $L_A$ is lineair (gegeven). We weten dat $L_A$ injectief is als (en slechts als) $ker(L_A) = \{\vec{0}\}$ geldt \footnote{Zie Stelling 4.29 p 156.}. $ker(L_A) = \{\vec{0}\}$ omdat $0$ geen eigenwaarde is van $L_A$ \footnote{Zie Stelling 6.49 p 249.} dus $E_0 = ker(L - 0\mathbb{I}_n) = \{\vec{0}\}$ \footnote{Zie Opmerking 5.15 p 188.}. Omdat $L_A$ injectief is, is $L_A$ ook surjectief en dus bijectief \footnote{Zie Gevolg 4.35 p 159.}.

\item
Uit het eerste deel van dit bewijs volgt dat $L_A$ de lengte van vectoren behoudt. We bewijzen nog dat $L_A$ niet enkel de orthogonaliteit maar zelfs het inproduct behoudt.
\[
\langle A\cdot X, A\cdot Y \rangle = X^T A^T A Y = X^T A^{-1} A Y = X^T Y = \langle X, Y \rangle
\]
(Merk op dat dit enkel geldt omdat $A$ orthogonaal is.)
Omdat $L_A$ de lengte en orthogonaliteit behoudt (en het beeld van een basis opnieuw een basis vormt \footnote{Zie Gevolg 4.3 p 130.}) is het beeld van een orthonormale basis nog steeds een orthonormale basis.
\end{enumerate}
\end{proof}

\subsection{Stelling 6.52 p 251}
Zij $A \in \mathbb{C}^{n\times n}$ een Hermetische matrix.
\subsubsection*{Te Bewijzen}
Er bestaan een unitaire matrix $P \in \mathbb{C}^{n\times n}$ zodat $P^{-1}AP = \overline{P}^TAP$ een diagonaalmatrix is.
\subsubsection*{Bewijs}
\begin{proof}

\end{proof}

\subsection{Stelling 6.56 p 252}
\subsubsection*{Te Bewijzen}
\subsubsection*{Bewijs}
\begin{proof}

\end{proof}

\subsection{Stelling 6.58 p 253}
Zij $L$ een lineaire transformatie van een inproductruimte $(\mathbb{R},V,+,\langle \cdot,\cdot \rangle)$.
\subsubsection*{Te Bewijzen}
\[
\text{ L is orthogonaal } \Leftrightarrow \forall v \in V: \Vert L(v) \Vert = \Vert v\Vert
\]
\subsubsection*{Bewijs}
\begin{proof}
Bewijs van een equivalentie.\\
\begin{itemize}
\item $\Rightarrow$
Omdat $L$ orthogonaal is geldt voor elke $v,w \in V$ het volgende.
\[
\langle L(v), L(w) \rangle = \langle v,w\rangle
\]
Kies nu een willekeurige $v \in V$ om het te bewijzen voor elke $v\in V$.
\[
\Vert L(v) \Vert = \sqrt{\langle L(v), L(v) \rangle} = \sqrt{\langle v, v \rangle} = \Vert v\Vert
\]

\item $\Leftarrow$
We gebruiken eenderzijds de lineariteit van $L$ en anderzijds de gegeven eigenschap dat $L$ de norm van elke vector $v \in V$ bewaart. Kies willekeurige $v,w \in V$ om het te bewijzen voor elk paar vectoren in $V$.
\begin{enumerate}
\item
\[
\Vert L(v+w) \Vert^2 = \Vert L(v)+L(w) \Vert^2 = \langle L(v)+L(w),L(v)+L(w) \rangle
\]
\[
= \langle L(v),L(v)+L(w) \rangle + \langle L(w),L(v)+L(w) \rangle
\]
\[
= \langle L(v),L(v) \rangle + \langle L(v),L(w) \rangle + \langle L(w),L(v) \rangle +\langle L(w),L(w) \rangle
\]
\[
= \Vert L(v)\Vert^2 + 2\langle L(v),L(w) \rangle +\Vert L(w)\Vert^2
\]
\[
= \Vert v\Vert^2 + 2\langle L(v),L(w) \rangle +\Vert w\Vert^2
\]
Al deze gelijkheden gelden simpelweg omwille van de definitie van het inproduct en de norm.\footnote{Zie Definitie 6.1 p 222.} \footnote{Zie Definitie 6.9 p 227.}. De laatste gelijkheid geldt omdat $L$ de norm van elke vector in $V$ bewaart.
\item
\[
\Vert L(v+w) \Vert^2 = \Vert v+w \Vert^2
\]
\[
= \langle v+w,v+w\rangle = \langle v,v+w\rangle + \langle w,v+w\rangle = \langle v,v\rangle + \langle v,w\rangle + \langle w,v\rangle + \langle w,w\rangle
\]
\[
= \Vert v\Vert^2 + 2\langle v,w \rangle +\Vert w\Vert^2
\]
\end{enumerate}
Voegen we deze redeneringen nu samen dan krijgen we de volgende gelijkheid.
\[
\Vert v\Vert^2 + 2\langle L(v),L(w) \rangle +\Vert w\Vert^2 = \Vert v\Vert^2 + 2\langle v,w \rangle +\Vert w\Vert^2
\]
\[
\langle L(v),L(w) \rangle =\langle v,w \rangle
\]
\end{itemize}
\end{proof}

\subsection{Propositie 6.59 p 254}
Zij $L$ een lineaire transformatie van een inproductruimte $(\mathbb{R},V,+,\langle \cdot,\cdot \rangle)$.
\subsubsection*{Te Bewijzen}
\begin{enumerate}
\item Elke (re\"eele) eigenwaarde van $L$ is $1$ of $-1$.
\item $L$ is injectief en als $V$ eindigdimensionaal is dan is $L$ ook een isomorfisme.
\end{enumerate}
\subsubsection*{Bewijs}
\begin{proof}
We kiezen een basis van $V$ en orthonormaliseren deze tot een basis $\beta$ \footnote{Zie Stelling 6.27 p 238.}. Als we $L$ voorstellen ten opzichte van $\beta$ met $A$ dan is $A$ een orthonormale matrix.
%TODO waarom
Nu kunnen we bewezen eigenschappen toepassen op $A$ \footnote{Zie Lemma 6.49 p 249}.
\end{proof}

\subsection{Stelling 6.60 p 254}
Zij $L$ een lineaire transformatie van een $n$-dimensionale inproductruimte $(\mathbb{R},V,+,\langle \cdot,\cdot \rangle)$.
Zij $A$ een matrix van $L$ ten opzichte van een orthonormale basis $\beta = e_1,e_2,...,e_n$.

\subsubsection*{Te Bewijzen}
\begin{center}
$L$ is orthogonaal $\Leftrightarrow$ $A$ is orthogonaal.
\end{center}

\subsubsection*{Bewijs}
\begin{proof}
Bewijs van een equivalentie.
\begin{itemize}
\item $\Rightarrow$\\
We beginnen met de definitie van een orthogonale lineaire transformatie.
\[
\langle L(v),L(w) \rangle = \langle v,w \rangle
\]
\[
X_{L(v)}^T \cdot X_{L(w)} = X_v^T \cdot X_w 
\]
\[
(A\cdot X_{v})^T \cdot (A\cdot X_{w}) = X_v^T \cdot X_w 
\]
\[
X_v^{T}\cdot A^T \cdot A \cdot X_{w} = X_v^T \cdot X_w 
\]
Deze gelijkheid kan enkel gelden als geldt dat $A^T \cdot A = \mathbb{I}_n$.
Dit houdt precies in dat $A$ orthogonaal is.

\item $\Leftarrow$\\
We beschouwen de co\"ordinaten van twee willekeurige vectoren $v,w \in V$ ten opzichte van $\beta$.
Dit zijn co\"ordinaatvectoren $X_v$ en $X_w$.
Nu geldt het volgende.
\[
\langle L(v),L(w) \rangle = X_{L(v)}^T \cdot X_{L(w)} = (A\cdot X_{v})^T \cdot (A\cdot X_{w}) = X_v^{T}\cdot A^T \cdot A \cdot X_{w}
\]
Omdat $A$ orthogonaal is geldt de volgende gelijkheid.
\[
 = X_v^{T}\cdot A^{-1} \cdot A \cdot X_{w} = X_v^T \cdot X_w  = \langle v,w \rangle
\]
Dit houdt precies in dat $L$ orthogonaal is.
\end{itemize}
%TODO waar is hier gebruikt dat de gegeven basis orthonormaal is?
\end{proof}

\subsection{Propositie 6.62 p 255}
Zij $L$ een symmetrische lineaire transformativ van een inproductruimte $(\mathbb{R},V,+,\langle \cdot,\cdot \rangle)$.
\subsubsection*{Te Bewijzen}
Eigenvectoren bij verschillende eigenwaarden van $L$ zijn orthogonaal.
\subsubsection*{Bewijs}
\begin{proof}
Bewijs door gefoefel.\\
Zij $v$ en $w$ eigenvectoren die respectievelijk bij verschillende eigenwaarden $\lambda$ en $\mu$ horen van $L$.
\[
\lambda \langle v,w \rangle = \langle \lambda v,w \rangle = \langle L(v),w \rangle = \langle v,L(w) \rangle = \langle v,\mu w \rangle = \mu \langle v,w \rangle
\]
Samengevat staat hierboven het volgende.
\[
\lambda \langle v,w \rangle =\mu \langle v,w \rangle
\]
\[
\lambda = \mu \vee \langle v,w \rangle = 0
\]
We hebben aangenomen dat $\lambda$ en $\mu$ verschillende eigenwaarden zijn dus moeten $v$ en $w$ orthogonaal zijn.
\end{proof}

\subsection{Stelling 6.64 p 255}
Zij $(\mathbb{R},V,+,\langle \cdot,\cdot \rangle)$ een $n$-dimensionale inproductruimte en $L$ een lineaire transformatie van $V$ met matrix $A$ ten opzichte van een orthonormale basis $\beta$ van $V$.

\subsubsection*{Te Bewijzen}
\begin{center}
$L$ is symmetrisch $\Leftrightarrow$ $A$ is symmetrisch.
\end{center}

\subsubsection*{Bewijs}
\begin{proof}
Bewijs van een equivalentie.\\
\begin{itemize}
\item $\Rightarrow$\\
We gebruiken de definitie van symmetrie van een lineaire transformatie \footnote{Zie Definitie 6.61 p 255.}.
Kies willekeurige $v,w \in V$. Zij $X_v$ de co\"ordinaatvector van $v$ ten opzichte van $\beta$.
\[
\langle L(v),w \rangle = \langle v,L(w)\rangle
\]
\[
X_{L(v)}^T \cdot X_w = X_v^T \cdot X_{L(w)}
\]
\[
(A \cdot X_v)^T \cdot X_w = X_v^T \cdot A X_w
\]
\[
 X_v^T \cdot A^T \cdot X_w = X_v^T \cdot A X_w
\]
Dit kan enkel kloppen als $A^T = A$ geldt en dat houdt precies in dat $A$ symmetrisch is.

\item $\Leftarrow$\\
Als $A$ symmetrisch is geldt $A = A^T$. Kies willekeurige $v,w \in V$. Zij $X_v$ de co\"ordinaatvector van $v$ ten opzichte van $\beta$.
\[
\langle L(v),w \rangle = X_{L(v)}^T \cdot X_w = (A \cdot X_v)^T \cdot X_w =  X_v^T \cdot A^T \cdot X_w 
\]
We gebruiken nu dat $A$ symmetrisch is.
\[
=  X_v^T \cdot A \cdot X_w = X_v^T \cdot A X_w = X_v^T \cdot X_{L(w)} = \langle v,L(w)\rangle
\]
Samengevat staat hierboven het volgende.
\[
\langle L(v),w \rangle = \langle v,L(w)\rangle
\]
Dit houdt precies in dat $L$ symmetrisch is.
\end{itemize}
\end{proof}

\subsection{Stelling 6.68 p 259}
Zij $A \in \mathbb{R}^{n\times n}$ en zij $m$ en $M$ respectievelijk de kleinste en de grootste eigenwaarde van $A$.
Beschouw de volgende kwadratische vorm  waarbij we $x$ interpreteren als kolomvector.
\[
Q:\mathbb{R}^n \rightarrow \mathbb{R}:x \mapsto Q(x) = x^TAx
\]

\subsubsection*{Te Bewijzen}
\[
m = min\{ Q(x)\ |\ x \in \mathbb{R}^n, \Vert x\Vert = \}
\]
\[
M = max\{ Q(x)\ |\ x \in \mathbb{R}^n, \Vert x\Vert = 1\}
\]
De $x_m$ waarvoor $Q(x_m)=m$ en $Q(x_M)=M$ zijn eigenvectoren. 

\subsubsection*{Bewijs}
\begin{proof}
Door de spectraalstelling voor symmetrische matrices weten we dat $A$ re\"ele eigenwaarden heeft.
Er bestaat dus een orthonormale basis $\beta$ van eigenvectoren voor $A$.
Er bestaat bijgevolg een orthogonale matrix $P$ zodat de volgende gelijkheden gelden.
\[
P^T\cdot A\cdot P = P^{-1}\cdot A\cdot P = \Lambda
\]
Hierboven is $\Lambda$ de diagonaalmatrix met de eigenwaarden $\lambda_1,\lambda_2,...,\lambda_n$ van $A$ op de hoofddiagonaal.
Bovendien is $P$ de matrix waarin de orthonormale eigenvectoren in kolommen staan.
We willen nu $X$ schrijven volgens $\beta$. Hier volgt de volgende uitdrukking uit.
\[
X = P\cdot Y
\]
Beschouw nu wat deze nieuwe informatie doet met de eerste drukking.
\[
X^T\cdot A \cdot X = Y^T\cdot P^{-1}\cdot A\cdot P\cdot Y  = Y^T\cdot \Lambda \cdot Y
\]
De originele quadratische vorm wordt nu de volgende.
\[
Q:\mathbb{R}^n \rightarrow \mathbb{R}: Y \mapsto Y^T\cdot \Lambda Y = \sum_{i=1}^n\lambda_iy_i^2
\]
Stel nu dat de eigenwaarden (en bijhorende eigenvectoren) gerangschikt staan in $\Lambda$ (respectievelijk $P$) zodat de volgende bewering geldt. 
Het is altijd mogelijk om de eigenwaarden zo gerangschikt te krijgen met hetzelfde resultaat.
\[
m=\lambda_1,\lambda_2,...,\lambda_n=M
\]
Nu geldt voor elke $v\in \mathbb{R}^n$ dat $\Vert v\Vert = 1 \Leftrightarrow \sum_{i=1}^ny_i^2 = 1$ omdat $\beta$ orthonormaal is.
Op de eenheidssfeer bereikt $Q$ als minimumwaarde $\lambda_1=m$ en als maximumwaarde $\lambda_n=M$. $x_m$ is dan $p_1$ en $x_M$ is dan $p_n$. Let wel op dat die $x_m$ en $x_M$ nog volgens de originele basis geschreven moeten worden.


\end{proof}


\end{document}